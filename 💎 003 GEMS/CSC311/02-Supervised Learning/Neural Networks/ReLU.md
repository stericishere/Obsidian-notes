## ReLU Overview:
$$ReLU(x)=max(0,x)$$
Relu basically turn negative input into 0
Why use Relu?
	Helps training converge faster compared to sigmoid/tanh

---
## Challenge:
<u><span style="background:#d2cbff">Solve:</span></u>
[[The Vanishing Gradient Problem]]
<u><span style="background:#d2cbff">Cause:</span></u>
[[Dying ReLU problem]]
