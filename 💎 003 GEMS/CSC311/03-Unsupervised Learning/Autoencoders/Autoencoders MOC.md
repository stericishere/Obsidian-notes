## ðŸ”¢ Table View: Autoencoder Components
**Unsupervised Learning**
**Autoencoder Training:** 
	Encode â†’ bottleneck â†’ decode â†’ minimize reconstruction error.
	

| Component       | Role                              | Example                               |
| --------------- | --------------------------------- | ------------------------------------- |
| Encoder         | Compress input to [[Latent code]] | 784 â†’ 32 neurons (MNIST digit images) |
| [[Latent code]] | Bottleneck representation         | 32-dimensional feature vector         |
| Decoder         | Reconstructs input                | 32 â†’ 784 neurons                      |
| Loss Function   | Measures reconstruction error     | MSE or Cross-Entropy                  |
| Training Data   | Inputs = Outputs                  | Image â†’ Image, Signal â†’ Signal        |
|                 |                                   |                                       |
> [!TIP]  
> Think of autoencoders as **unsupervised feature learners**: instead of labels, the network learns structure directly from the input.

- [[01 Feed-forward neural net for input prediction]]
- [[02 Bottleneck layer]]
- [[Linear Autoencoders and PCA relationship]]
- [[04 Nonlinear Autoencoders]]
